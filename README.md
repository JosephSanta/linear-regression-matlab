# ğŸ“ˆ Linear Regression from Scratch in MATLAB

This project is part of my **AI & Machine Learning portfolio**.  
It demonstrates how to implement **linear regression from scratch** using MATLAB, including both **gradient descent optimization** and the **normal equation**.  

ğŸ‘‰ Why it matters: Gradient descent, applied here to a simple dataset, is the same optimization method used to train **neural networks and modern AI models**.

---

## ğŸš€ Highlights
- Implemented **Linear Regression**:
  - One variable (predicting food truck profits).
  - Multiple variables (predicting house prices).
- Built all components **from scratch**:
  - Cost functions (`J(Î¸)`).
  - Gradient descent (single and multivariable).
  - Feature normalization.
  - Normal equation closed-form solution.
- Compared optimization methods:
  - Gradient Descent vs. Normal Equation â†’ same result âœ…
- Visualized:
  - Training data + regression line.
  - Cost function contours & surface.
  - Convergence of `J(Î¸)` for different learning rates.

---

## ğŸ“Š Results
- **Single variable regression**:  
  - Î¸ â‰ˆ [-3.63, 1.17]  
  - Predictions:  
    - 35,000 population â†’ \$4,520 profit  
    - 70,000 population â†’ \$45,342 profit  

**Multivariable regression (house prices)**:  
  - Gradient Descent (Î±=0.03, 1000 iterations): â‰ˆ \$293,081  
  - Normal Equation: â‰ˆ \$293,081  


![alt text](image.png)
![alt text](image-1.png)
![alt text](image-2.png)
![alt text](image-3.png)
---

## ğŸ§  Skills Demonstrated
- Machine Learning Fundamentals: linear regression, cost functions.  
- Optimization: gradient descent and hyperparameter tuning.  
- Data Preprocessing: feature scaling and normalization.  
- Numerical Computing: vectorized implementation in MATLAB.  
- Model Evaluation: comparing iterative vs closed-form solutions.  

---

## ğŸ“‚ Repository Structure

```

â”œâ”€â”€ MATLAB/
â”‚ â”œâ”€â”€ calculeCosto.m
â”‚ â”œâ”€â”€ calculeCostoMulti.m
â”‚ â”œâ”€â”€ calentamiento.m
â”‚ â”œâ”€â”€ descensoXGradiente.m
â”‚ â”œâ”€â”€ descensoXGradienteMulti.m
â”‚ â”œâ”€â”€ ecuacionNormal.m
â”‚ â”œâ”€â”€ ej1_multi.m
â”‚ â”œâ”€â”€ ej1.m
â”‚ â”œâ”€â”€ ej1data1.txt
â”‚ â”œâ”€â”€ ej1data2.txt
â”‚ â”œâ”€â”€ normaliceCaracteristicas.m
â”‚ â””â”€â”€ plotData.m
â”‚
â”œâ”€â”€ ex1.pdf # Task statement
â”œâ”€â”€ IA_GR40_Tarea1_Regresion... # Final report (PDF)
â”‚
â”œâ”€â”€ image-1.png
â”œâ”€â”€ image-2.png
â”œâ”€â”€ Fimage-3.png
â”‚
â””â”€â”€ README.md

```
---

## ğŸ”® Next Steps
- Extend to **logistic regression** for classification tasks.  
- Explore advanced optimizers: **SGD, Adam**.  
- Implement in **Python (NumPy / scikit-learn)** to connect with modern ML stacks.  

---

## ğŸ‘¨â€ğŸ’» Author
**Joseph SantamarÃ­a Castro**  
AI & Machine Learning student at *Instituto TecnolÃ³gico de Costa Rica*  
